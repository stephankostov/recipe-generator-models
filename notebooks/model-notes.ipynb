{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Own Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions to ask before:\n",
    "\n",
    "We are looking for predictions of only the masked tokens, as a set of probabilities (logits) for every possible token. ie. `[batch_size, n_masks, n_tokens]`. How do the models achieve this? If understood I could implement this into the PyTorch easily.\n",
    "\n",
    "- https://github.com/dhlee347/pytorchic-bert/tree/master\n",
    "- https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py#L41\n",
    "    - requires reshifting of data generation (which is easily possible) but not sure if it will help improve results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions when looking through BERT to try to figure it out. \n",
    "\n",
    "Where is the masking done? Is it done in the data loading stage, or is it done in the model itself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedSelfAttention(nn.Module):\n",
    "    \"\"\" Multi-Headed Dot Product Attention \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.proj_q = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.proj_k = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.proj_v = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.drop = nn.Dropout(cfg.p_drop_attn)\n",
    "        self.scores = None # for visualization\n",
    "        self.n_heads = cfg.n_heads\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        x, q(query), k(key), v(value) : (B(batch_size), S(seq_len), D(dim))\n",
    "        mask : (B(batch_size) x S(seq_len))\n",
    "        * split D(dim) into (H(n_heads), W(width of head)) ; D = H * W\n",
    "        \"\"\"\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q, k, v = self.proj_q(x), self.proj_k(x), self.proj_v(x)\n",
    "        q, k, v = (split_last(x, (self.n_heads, -1)).transpose(1, 2)\n",
    "                   for x in [q, k, v])\n",
    "        # (B, H, S, W) @ (B, H, W, S) -> (B, H, S, S) -softmax-> (B, H, S, S)\n",
    "        scores = q @ k.transpose(-2, -1) / np.sqrt(k.size(-1))\n",
    "        if mask is not None:\n",
    "            mask = mask[:, None, None, :].float()\n",
    "            scores -= 10000.0 * (1.0 - mask)\n",
    "        scores = self.drop(F.softmax(scores, dim=-1))\n",
    "        # (B, H, S, S) @ (B, H, S, W) -> (B, H, S, W) -trans-> (B, S, H, W)\n",
    "        h = (scores @ v).transpose(1, 2).contiguous()\n",
    "        # -merge-> (B, S, D)\n",
    "        h = merge_last(h, 2)\n",
    "        self.scores = scores\n",
    "        return h\n",
    "\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    \"\"\" FeedForward Neural Networks for each position \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg.dim, cfg.dim_ff)\n",
    "        self.fc2 = nn.Linear(cfg.dim_ff, cfg.dim)\n",
    "        #self.activ = lambda x: activ_fn(cfg.activ_fn, x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (B, S, D) -> (B, S, D_ff) -> (B, S, D)\n",
    "        return self.fc2(gelu(self.fc1(x)))\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer Block \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadedSelfAttention(cfg)\n",
    "        self.proj = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.norm1 = LayerNorm(cfg)\n",
    "        self.pwff = PositionWiseFeedForward(cfg)\n",
    "        self.norm2 = LayerNorm(cfg)\n",
    "        self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        h = self.attn(x, mask)\n",
    "        h = self.norm1(x + self.drop(self.proj(h)))\n",
    "        h = self.norm2(h + self.drop(self.pwff(h)))\n",
    "        return h\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\" Transformer with Self-Attentive Blocks\"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.embed = Embeddings(cfg)\n",
    "        self.blocks = nn.ModuleList([Block(cfg) for _ in range(cfg.n_layers)])\n",
    "\n",
    "    def forward(self, x, seg, mask):\n",
    "        h = self.embed(x, seg)\n",
    "        for block in self.blocks:\n",
    "            h = block(h, mask)\n",
    "        return h"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
